{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d70b90-86b6-46ae-a61c-d75a6822ff88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T11:27:21.982414Z",
     "start_time": "2024-03-21T11:27:19.411012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 10:54:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:19:00.0 Off |                  N/A |\n",
      "| 30%   28C    P5              83W / 350W |     10MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   27C    P5              78W / 350W |     10MiB / 24576MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc2a266-40ce-4575-b93c-e16ced929bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import notebook_launcher\n",
    "# from accelerate imporself.model_.t Accelerator\n",
    "\n",
    "# def test_loop():\n",
    "#     accelerator = Accelerator()\n",
    "#     print(accelerator.state)\n",
    "# notebook_launcher(test_loop, [], num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807b865f-ad98-4067-a15d-894d482bc9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/usoltsev/study/repositories/rugpt-memory/')\n",
    "\n",
    "# assert torch.cuda.is_available()\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b28f573-e460-489b-817c-82f2700caf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ltm_gpt.ltm_gpt import LTM_GPT\n",
    "from src.utils.train_config import load_config\n",
    "from src.models.load_base_model import load_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5de65f4-cd46-4ec0-9888-37c0fd2bb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "    print(batch)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        # print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1)  # .argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        # print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        # print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "        break\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4d12bf-62e4-466f-93f8-8b6fe7db15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c306cad-2dfa-45d7-a3ef-e927771e57c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-04-01 10:28:43,327: INFO] Loading base model (load_base_model.py:9)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n",
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "main_config = load_config('/home/usoltsev/study/repositories/rugpt-memory/configs/finetuning_codeparrot.yml')\n",
    "\n",
    "model, tokenizer = load_base_model(main_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8dce78-c5b6-408c-9e1e-c15308866dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device_for_ltm_layers = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e61c1a3-ab19-4b5a-a6c3-123a6b3be456",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_blocks_with_memory = 2\n",
    "\n",
    "model = LTM_GPT(\n",
    "    model,\n",
    "    cnt_blocks_with_memory=cnt_blocks_with_memory,\n",
    "    device=device_for_ltm_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cb4e3c-c0d4-46e0-9f06-a000c14efd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformer.h[-cnt_blocks_with_memory:].parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ac12dd-3880-4a65-a006-3aaa665b468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 10:31:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:19:00.0 Off |                  N/A |\n",
      "| 30%   31C    P8              21W / 350W |  12865MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   32C    P2              56W / 350W |  13553MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    691072      C   ...iniforge3/envs/rugpt_dev/bin/python    12850MiB |\n",
      "|    1   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A    691072      C   ...iniforge3/envs/rugpt_dev/bin/python    13538MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee5e55d-0497-47a6-b72b-affa0ddf4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[33076]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "{'input_ids': tensor([[34958]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "{'input_ids': tensor([[29631]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "{'input_ids': tensor([[  89, 2286]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "{'input_ids': tensor([[1271]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "{'input_ids': tensor([[9949]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "{'input_ids': tensor([[23652,  1028]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n",
      "3\n",
      "type(query): <class 'torch.Tensor'>\n",
      "type(query): <class 'torch.Tensor'>\n",
      "[',', ',', ',', 'ry-', ',', ',', 'ch:']\n"
     ]
    }
   ],
   "source": [
    "prompts = ['import', 'from', 'while', 'try', 'if', 'for',\n",
    "           'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "after_finetuning_samples = []\n",
    "for prompt in prompts:\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, torch.device('cuda:0'), MAX_STEPS))\n",
    "print(after_finetuning_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf267ef-069f-4699-b2f3-40371c852f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4e662-2c2e-467c-b165-237608f3704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea5165-9cdc-46e1-9c28-dcbcdd95765b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0fa7c2-0b98-43f5-8ded-d0944be99a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9619f71-27d3-42f6-aa1f-814c87b8a197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab66893-01f8-489e-8c81-5c9e5408de05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92f821-5535-4b22-9dde-ba4113c49c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67411d-024b-4f19-8f5d-0620f6affaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6213a-2b2a-4847-82fb-4ed6f13e956e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a1fc4-6551-43e6-9ef5-134bdcbe01b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fbafa-2d2c-4b3f-a075-bf62900b3ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f11a24-a771-46e6-9bca-f71503a68ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[33076]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n",
      "3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m after_finetuning_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 8\u001b[0m     after_finetuning_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcustom_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(after_finetuning_samples)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mcustom_generate\u001b[0;34m(prompt, model, device, max_steps)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     probs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnan_to_num(nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# .argmax(-1).reshape(1, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/study/repositories/rugpt-memory/src/models/ltm_gpt/ltm_gpt.py:115\u001b[0m, in \u001b[0;36mLTM_GPT.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_ltm_blocks:\n\u001b[1;32m    114\u001b[0m     block\u001b[38;5;241m.\u001b[39mupdate_memory(memory)\n\u001b[0;32m--> 115\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mmodel_parallel:\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/study/repositories/rugpt-memory/src/models/ltm_gpt/ltm_gpt2_block.py:68\u001b[0m, in \u001b[0;36mLTMGPT2Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# MultiHead Attention\u001b[39;00m\n\u001b[1;32m     67\u001b[0m key, value \u001b[38;5;241m=\u001b[39m memory, memory\n\u001b[0;32m---> 68\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Norm & Concat\u001b[39;00m\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m residual\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/activation.py:1116\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(attn_mask))\n\u001b[1;32m   1113\u001b[0m    \u001b[38;5;129;01mor\u001b[39;00m (key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(key_padding_mask)):\n\u001b[1;32m   1114\u001b[0m     why_not_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating-point masks are not supported for fast path.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1116\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m   1118\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m   1119\u001b[0m     mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1120\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     target_type\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1124\u001b[0m )\n\u001b[1;32m   1126\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m   1127\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1128\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     check_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1133\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "prompts = ['import', 'from', 'while', 'try', 'if', 'for',\n",
    "           'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "after_finetuning_samples = []\n",
    "for prompt in prompts:\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, torch.device('cuda:0'), MAX_STEPS))\n",
    "print(after_finetuning_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b42a0d-c225-4e75-9f49-80a4dd9492a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb3181-2bbe-42b6-9c28-cd0e9689501c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5ed57-5fa6-490b-b255-21231561bf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e1ae5-12c4-473a-9e71-df52cd7692fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896da6c7-9391-4ddd-828f-5f921794024c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bd12d-d638-4622-86f4-deed5fe71640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d26095c5-6802-40e3-acfd-de26a3fb6670",
   "metadata": {},
   "source": [
    "### Current problem: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)\n",
    "Solution: create new layers on cuda:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627b1ac1-5c05-40eb-af2b-6fbb91df23d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 31 19:57:41 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:19:00.0 Off |                  N/A |\n",
      "| 30%   29C    P2              67W / 350W |  16485MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   28C    P2             100W / 350W |  12353MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    598973      C   ...iniforge3/envs/rugpt_dev/bin/python    16470MiB |\n",
      "|    1   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A    598973      C   ...iniforge3/envs/rugpt_dev/bin/python    12338MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373924fa-c6c4-40cf-aa1c-6ab336cc1ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[33076]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m after_finetuning_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 8\u001b[0m     after_finetuning_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcustom_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(after_finetuning_samples)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mcustom_generate\u001b[0;34m(prompt, model, device, max_steps)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     probs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnan_to_num(nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# .argmax(-1).reshape(1, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/study/repositories/rugpt-memory/src/models/ltm_gpt/ltm_gpt.py:77\u001b[0m, in \u001b[0;36mLTM_GPT.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m---> 77\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Init memory as hidden_states from 37 layers\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# BaseModelOutputWithPastAndCrossAttentions(\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#     last_hidden_state=hidden_states,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#     cross_attentions=all_cross_attentions,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[1;32m    388\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 389\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    391\u001b[0m         hidden_states,\n\u001b[1;32m    392\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)"
     ]
    }
   ],
   "source": [
    "prompts = ['import', 'from', 'while', 'try', 'if', 'for',\n",
    "           'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "after_finetuning_samples = []\n",
    "for prompt in prompts:\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "print(after_finetuning_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2dc1b8-1670-4fdc-b544-f418d55370aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3722850-19d8-4717-a249-241ef8561a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e33ec5-8b7c-4791-b2f0-f87abad11844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08c47866-070f-4288-b23f-8288289628c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "2\n",
      "\u001b[32m[2024-03-27 10:06:45,009: INFO] Loading base model (load_base_model.py:9)\u001b[0m\n",
      "2\n",
      "\u001b[32m[2024-03-27 10:06:45,017: INFO] Loading base model (load_base_model.py:9)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                    | 0/6 [00:00<?, ?it/s]/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards:  33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 2/6 [00:05<00:11,  2.88s/it]\n",
      "Loading checkpoint shards:  33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 2/6 [00:05<00:11,  2.92s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "An issue was found when launching the training: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/utils/launch.py\", line 624, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_2854751/2966003378.py\", line 10, in training_loop1\n    model, tokenizer = load_base_model(main_config)\n  File \"/home/usoltsev/study/repositories/rugpt-memory/src/models/load_base_model.py\", line 14, in load_base_model\n    model = AutoModelForCausalLM.from_pretrained(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3502, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3926, in _load_pretrained_model\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 805, in _load_state_dict_into_meta_model\n    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 399, in set_module_tensor_to_device\n    new_value = value.to(device)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 101.00 MiB is free. Process 2859156 has 12.08 GiB memory in use. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 11.24 GiB is allocated by PyTorch, and 2.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/launchers.py:201\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:202\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:163\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/utils/launch.py\", line 624, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_2854751/2966003378.py\", line 10, in training_loop1\n    model, tokenizer = load_base_model(main_config)\n  File \"/home/usoltsev/study/repositories/rugpt-memory/src/models/load_base_model.py\", line 14, in load_base_model\n    model = AutoModelForCausalLM.from_pretrained(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3502, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3926, in _load_pretrained_model\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 805, in _load_state_dict_into_meta_model\n    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 399, in set_module_tensor_to_device\n    new_value = value.to(device)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 101.00 MiB is free. Process 2859156 has 12.08 GiB memory in use. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 11.24 GiB is allocated by PyTorch, and 2.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 45\u001b[0m\n\u001b[1;32m     10\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m load_base_model(main_config)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     cnt_blocks_with_memory = 2\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# model = LTM_GPT(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#     print(after_finetuning_samples)\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loop1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/launchers.py:211\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    205\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA has been initialized before the `notebook_launcher` could create a forked subprocess. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis likely stems from an outside import causing issues once the `notebook_launcher()` is called. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease review your imports and test them when running the `notebook_launcher()` to identify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich one is problematic and causing CUDA to be initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn issue was found when launching the training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_mps_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: An issue was found when launching the training: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/utils/launch.py\", line 624, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_2854751/2966003378.py\", line 10, in training_loop1\n    model, tokenizer = load_base_model(main_config)\n  File \"/home/usoltsev/study/repositories/rugpt-memory/src/models/load_base_model.py\", line 14, in load_base_model\n    model = AutoModelForCausalLM.from_pretrained(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3502, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3926, in _load_pretrained_model\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 805, in _load_state_dict_into_meta_model\n    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n  File \"/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 399, in set_module_tensor_to_device\n    new_value = value.to(device)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 101.00 MiB is free. Process 2859156 has 12.08 GiB memory in use. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 11.24 GiB is allocated by PyTorch, and 2.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from accelerate import Accelerator\n",
    "\n",
    "def training_loop1(unused, unused2):\n",
    "    accelerator = Accelerator()\n",
    "    print(accelerator.state.num_processes)\n",
    "    \n",
    "    main_config = load_config('/home/usoltsev/study/repositories/rugpt-memory/configs/finetuning_codeparrot.yml')\n",
    "\n",
    "    model, tokenizer = load_base_model(main_config)\n",
    "    \n",
    "#     cnt_blocks_with_memory = 2\n",
    "\n",
    "    # model = LTM_GPT(\n",
    "    #     model,\n",
    "    #     cnt_blocks_with_memory=cnt_blocks_with_memory\n",
    "    # )\n",
    "    \n",
    "#     model = accelerator.prepare(model)\n",
    "    \n",
    "    \n",
    "#     for param in model.transformer.h[-cnt_blocks_with_memory:].parameters():\n",
    "#         param.requires_grad=True\n",
    "#         # param.data = param.data.to(torch.float32)\n",
    "\n",
    "#     for param in model.transformer.ln_f.parameters():\n",
    "#         param.requires_grad=True\n",
    "#         # param.data = param.data.to(torch.float32)\n",
    "\n",
    "#     for param in model.lm_head.parameters():\n",
    "#         param.requires_grad=True\n",
    "#         # param.data = param.data.to(torch.float32)\n",
    "\n",
    "#     prompts = ['import', 'from', 'while', 'try', 'if', 'for',\n",
    "#                'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "#     MAX_STEPS = 100\n",
    "\n",
    "#     device = accelerator.device\n",
    "\n",
    "#     after_finetuning_samples = []\n",
    "#     for prompt in prompts:\n",
    "#         after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "#     print(after_finetuning_samples)\n",
    "notebook_launcher(training_loop1, (None, None), num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0f02b-68db-4e6d-a2a5-15fd7e03cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(unused, unused2):\n",
    "    accelerator = Accelerator()\n",
    "    print(accelerator.state.num_processes)\n",
    "\n",
    "notebook_launcher(test, (None, None), num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebeab3f-e0c2-4556-ba45-62e40f9f6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c8b2f-73bb-4ebb-8fb2-c0c9f0dc12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(training_loop, (None, None), num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f091b1b-2e2a-4544-baf2-6fa64557ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8582c2-6271-4f71-9752-33b26eff1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba list | grep accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6af8a8-8a24-44db-b226-907ae88b7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba remove --yes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10dfb2-a8f1-44bf-92f3-c796cb4c438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e15ef9-9c4f-4445-890a-fd8e2ba77d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57134631-2a5a-4c20-b68e-edad1b0e0b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7cfbe-2698-4e4b-83c2-3d56dd4424cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_config = load_config('/home/usoltsev/study/repositories/rugpt-memory/configs/finetuning_codeparrot.yml')\n",
    "\n",
    "model, tokenizer = load_base_model(main_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71809a-5d10-4471-bae6-ff738da2e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e9dc2-f0e1-4a1f-b6f2-6df62625a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_blocks_with_memory = 2\n",
    "\n",
    "model = LTM_GPT(\n",
    "    model,\n",
    "    cnt_blocks_with_memory=cnt_blocks_with_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33771e-46dc-439d-a1de-4696dd5eb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b6f0e-f9b5-4229-806f-a24de4ad1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a5aca-7a0c-4f9f-bf53-3f825f8a6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# each GPU creates a string\n",
    "message=[ f\"Hello this is GPU {accelerator.process_index}\" ] \n",
    "\n",
    "# collect the messages from all GPUs\n",
    "messages=gather_object(message)\n",
    "\n",
    "# output the messages only on the main process with accelerator.print() \n",
    "accelerator.print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f6c2c-249c-4950-a135-2a45f13208e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.process_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23b5f8-26f1-48d4-8d12-06bec59deac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.state.num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8915733-306b-4ef1-bc0c-665d9c215bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a348d13-9829-45cb-b7bc-bcc2ebe109c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0aa5a6-28e7-4810-a267-637f96065a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a1ac2-e9b1-4b37-9a90-16e884e63539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b82a2-cc63-4086-8af7-304425c8ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1697c-8ce9-433c-959a-83919701f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e34f4-1961-448b-99e2-414aa73442b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a4a52-f414-4290-b03f-998c130f85b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74407825-87ce-4a61-aec4-64e17a2daa96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49f32a-ed46-4c05-857f-8e867ec7f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb6bda-617c-4d5a-a488-280effaabf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde617f-8595-4bb7-8225-5fe4a3bd08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf5eca-32d2-464f-a801-8e85151ef583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ea34d-c0b4-4959-97d8-4f2f7a407ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0c385-b355-428b-b9df-64c73ca220a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509b6f6-aefe-44bd-91e3-4c48a94c089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd0044-3688-4b3b-b86a-8c149314506e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f079810-2f82-484e-82e2-34bed3fb682b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54711b18-434d-44d0-907c-1498f28c1692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4544f98-dbb8-4b8c-b031-219982add385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d127e-a0e1-4a2c-9eb9-dabcf168688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformer.h[-cnt_blocks_with_memory:].parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "prompts = ['import', 'from', 'while', 'try', 'if', 'for',\n",
    "           'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "    print(batch)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        # print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1)  # .argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        # print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        # print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "        break\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "after_finetuning_samples = []\n",
    "for prompt in prompts:\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "print(after_finetuning_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471d514-48f2-4aa7-b23a-0c33acbea02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_codeparrot_dataset import load_codeparrot_dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "dataset = load_codeparrot_dataset(tokenizer)\n",
    "\n",
    "# logger.info('Initializing tensorboard')\n",
    "tensorboard_logs_dir = f'logs/tensorboard/{main_config.exp_name}/'\n",
    "tensorboard_writer = SummaryWriter(tensorboard_logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb97ca-5dfa-4d44-9f7d-dc34f0e2bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info('Training')\n",
    "model._hf_peft_config_loaded = True # silence warnings - for research it is ok\n",
    "model.config.use_cache = False # silence warnings from torch\n",
    "\n",
    "trainer_args = main_config.trainer_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515d74e-9a73-48f3-ad0f-a804d455a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6a98c-bdfc-4c0e-bc75-04f7711224b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18d118-3353-4b4b-a2db-83ead97478ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import TensorBoardCallback\n",
    "trainer = accelerator.prepare(transformers.Trainer(\n",
    "    model=model, train_dataset=dataset['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=trainer_args.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=trainer_args.gradient_accumulation_steps,\n",
    "        warmup_steps=trainer_args.warmup_steps,\n",
    "        max_steps=trainer_args.max_steps,\n",
    "        learning_rate=trainer_args.learning_rate,\n",
    "        fp16=trainer_args.fp16,\n",
    "        logging_steps=trainer_args.logging_steps,\n",
    "        output_dir=f'checkpoints/{main_config.exp_name}/outputs',\n",
    "        report_to=trainer_args.report_to,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[TensorBoardCallback(tb_writer=tensorboard_writer)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd98813-009d-4a5a-ae34-aa89a15501a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f6b6f-a2f0-4b24-b711-c6a6f7bad126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a1e58-6ef7-4f10-b731-0bba6a8c827e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d89a7-deb4-411f-928c-4bc1ead21c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b21fd3-262f-40e8-bf08-7a1e093306e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ai-forever/ruGPT-3.5-13B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True, \n",
    "    cache_dir=\"/home/usoltsev/study/repositories/rugpt-memory/checkpoints/base/huggingface/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e42f0-c970-43dd-884c-23416616e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f426fc-5d30-4fa9-a948-df28c5242568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112129a-154f-4ff1-8dfd-11ff9ad8f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d721d-12c2-47bc-a338-75f068cd384b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf5795-c261-4671-b0b8-cf375d5279bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71636e-70df-48a9-907e-81713ae799a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ca59e-0f38-4936-b167-dca71157e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization \n",
    "model.gradient_checkpointing_enable() # memory optimization: only store a small subset of activations, re-compute the rest.\n",
    "model.enable_input_require_grads() # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
    "class CastOutputToFloat(torch.nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head) # cast model ouputs to unfuct the top-k sampler\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfd939-88e9-4a5d-ae55-69adf7d730ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    \"\"\" DenseNetwork layer(FeedForward in original paper) \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim=5120,\n",
    "        hidden_size=10240, \n",
    "        dtype=torch.float32,\n",
    "        initialize_with_zeros=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.ln1 = nn.Linear(self.embed_dim, self.hidden_size, dtype=self.dtype)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(self.hidden_size, self.embed_dim, dtype=self.dtype)\n",
    "        \n",
    "        if initialize_with_zeros:\n",
    "            nn.init.zeros_(self.ln1.weight)\n",
    "            nn.init.zeros_(self.ln1.bias)\n",
    "            nn.init.zeros_(self.ln2.weight)\n",
    "            nn.init.zeros_(self.ln2.bias)\n",
    "    \n",
    "    def forward(self, x): # x: (sentence_length, batch_size, self.embed_dim)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c3b9d-d868-4da9-9576-bb0b14271371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTMGPT2Block(nn.Module):\n",
    "    \"\"\" Custom LTMGPT2Block layer with memory \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        gpt2_block,\n",
    "        num_heads=4,\n",
    "        attn_dropout=0.1,\n",
    "        dense_network_hidden_size=10240,\n",
    "        dtype=torch.float32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gpt2_block = gpt2_block\n",
    "        \n",
    "        self.embed_dim = self.gpt2_block.ln_1.normalized_shape[0]\n",
    "        self.dense_network_hidden_size = dense_network_hidden_size\n",
    "        \n",
    "        assert dtype in [torch.float16, torch.float32]\n",
    "        \n",
    "        # self.memory: ( , , ) / (target_sentence_length, batch_size, self.embed_dim) (5120) | torch.FloatTensor / nn.Embedding\n",
    "        self.memory = None\n",
    "        \n",
    "        # goal: convert memory from ( , , ) to (source_sentence_length, batch_size, self.embed_dim)\n",
    "        self.dense_network1 = DenseNetwork(\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_size=self.dense_network_hidden_size, \n",
    "            dtype=dtype,\n",
    "            initialize_with_zeros=False\n",
    "        )\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention( # TODO masked ????\n",
    "            embed_dim=self.embed_dim, \n",
    "            num_heads=num_heads, \n",
    "            dropout=attn_dropout,\n",
    "            batch_first=False,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        self.dense_network2 = DenseNetwork(\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_size=self.dense_network_hidden_size, \n",
    "            dtype=dtype,\n",
    "            initialize_with_zeros=True\n",
    "        )\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x): # x: (sentence_length, batch_size, self.embed_dim)\n",
    "        assert not self.memory\n",
    "        \n",
    "        # TransformerBlock\n",
    "        query = self.gpt2_block(x) # query: (sentence_length, batch_size, self.embed_dim)\n",
    "        residual = query\n",
    "        \n",
    "        # DenseNetowork\n",
    "        memory = self.dense_network1(self.memory)\n",
    "        \n",
    "        # MultiHead Attention\n",
    "        key, value = memory, memory\n",
    "        x, _ = self.attn(\n",
    "            query=query, \n",
    "            key=key, \n",
    "            value=value\n",
    "        )\n",
    "        \n",
    "        # Norm & Concat\n",
    "        x = x + residual\n",
    "        if self.dtype == torch.float16:\n",
    "            x = self.ln1(x.float()).type(torch.float16)\n",
    "        else:\n",
    "            x = self.ln1(x)\n",
    "        \n",
    "        # DenseNetowork initialized with zeroes\n",
    "        x = self.dense_network2(x)\n",
    "        \n",
    "        # Norm & Concat\n",
    "        x = x + residual\n",
    "        if self.dtype == torch.float16:\n",
    "            x = self.ln2(x.float()).type(torch.float16)\n",
    "        else:\n",
    "            x = self.ln2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def update_memory(new_memory):\n",
    "        self.memory = new_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653cf5f-9dda-4cc8-aa0a-6716ed36bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18b069-0212-4846-a4e0-5dc30f6d3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce9e95-25ac-4719-b43e-4f2876e46c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer#.output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed359c8c-60ce-4f95-8319-ec9ecbaf03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.config.output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9b6c1-c4a3-4ef7-ac83-6074754fe42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.config.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee1899-c0db-48ad-9d16-25cee60dd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.config.output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2d0e-e043-49f0-8928-265b24b0a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b4899-b087-48e8-b5b4-c5ff5639614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65df8fa-112e-4ce7-8fc3-902d4ffae236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from typing import Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e453079-a7c2-4b42-84d2-8a5f3baa9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTM_GPT(GPT2LMHeadModel):\n",
    "    \"\"\" Custom LTM GPT2 layer with memory \"\"\"\n",
    "    def __init__(self, model_: GPT2LMHeadModel, cnt_blocks_with_memory=2):\n",
    "        super().__init__(model.config)\n",
    "        print(type(model_))\n",
    "        self.base_model = model_\n",
    "        print(type(self.base_model))\n",
    "        self.transformer = model.transformer\n",
    "        self.transformer.h = model_.transformer.h[:-cnt_blocks_with_memory]\n",
    "        \n",
    "        self.transformer_ltm_blocks = nn.ModuleList([\n",
    "            LTMGPT2Block(model_.transformer.h[-cnt_blocks_with_memory+i]) for i in range(cnt_blocks_with_memory)\n",
    "        ])\n",
    "        \n",
    "        self.lm_head = model_.lm_head\n",
    "\n",
    "        # Model parallel\n",
    "        # self.model_parallel = False\n",
    "        # self.device_map = None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        # self.post_init()\n",
    "    \n",
    "    # @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
    "    # @add_code_sample_docstrings(\n",
    "    #     checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "    #     output_type=CausalLMOutputWithCrossAttentions,\n",
    "    #     config_class=_CONFIG_FOR_DOC,\n",
    "    # )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.base_model.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        \n",
    "        # Init memory as hidden_states from 37 layers\n",
    "        # BaseModelOutputWithPastAndCrossAttentions(\n",
    "        #     last_hidden_state=hidden_states,\n",
    "        #     past_key_values=presents,\n",
    "        #     hidden_states=all_hidden_states,\n",
    "        #     attentions=all_self_attentions,\n",
    "        #     cross_attentions=all_cross_attentions,\n",
    "        # )    \n",
    "        \n",
    "        print(len(transformer_outputs))\n",
    "        memory = transformer_outputs[2][37]\n",
    "        \n",
    "        for block in self.transformer_ltm_blocks:\n",
    "            block.update_memory(memory)\n",
    "            hidden_states = block(hidden_states)\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.base_model.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84c847-4af6-4bb2-85dd-84bf88fe58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca888cf-b28a-42ec-a34b-17973ed45244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b11797-eb55-40d9-ab70-8cf0fac86caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f4cc8-11dd-4f02-abfe-f1f4850a512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3530d76-79da-48f2-b3e0-979d0dc28688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LTM_GPT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e18dd6-6c01-45b3-bcfd-6ebc8dfdcf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac44d17-c996-4a8b-99ef-f74ce23451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruGPT-3.5-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e589cab-0e0d-49cc-94e1-150bd924db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611d769-a31d-46bf-a574-cd6711eac9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =  ['import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    print(tokenizer(prompt, return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1e103-a121-44e0-ac73-54ae58d2734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "    print(batch)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        #print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1) #.argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        #print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        #print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "        break\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b26bc3-3763-4e2e-ab99-253ab90690e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_finetuning_samples = []\n",
    "for prompt in tqdm(prompts):\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "after_finetuning_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cb81d-971c-4240-8aa6-1ce689915863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427b55b-1040-4532-b0b3-8e12810c21a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be29ade-27b2-485a-9b05-60f4c4c5b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330d6bb-50f3-4a8f-85ff-d141c88ba309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5bf2d-e0b7-461e-9773-624b5f343856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e97d3f-f8eb-4eb4-8346-1ebf58555c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0f4b6-f149-4ff0-90d1-d40e514278af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03486b6-ae21-4696-9656-019798ab9c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ec581-c299-4f51-8468-f84c377e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f7938-6460-4055-820f-80905cc302f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebee22e-8aaf-48c9-b8b2-34dda72ffa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec01194-2d55-41e2-97f4-1a7730ffb255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289ae8-1011-4422-b1e6-d5c69eef5642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95210ae0-4f18-4e33-abe5-23016e7c0be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046e5a0-4dba-450d-adc8-b7158d297805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa818dbd-d310-4aca-b57b-c1c4e265b911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b669a-7646-4d8a-8fa8-c0b6d429ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bdad0-d8c3-47c9-8665-5a1a83a02783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036148b-45f2-49fc-9df4-81059f54f54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136f161-f03d-43c4-8227-0ae0a64ea049",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-3 + i for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c220098-1b3c-4ca7-93a5-c5e650817e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from typing import Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31024f97-4192-4b1b-958b-111b6112b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n",
    "            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n",
    "            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n",
    "            sequence tokens in the vocabulary.\n",
    "\n",
    "            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n",
    "            `input_ids`.\n",
    "\n",
    "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        past_key_values (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`):\n",
    "            Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n",
    "            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n",
    "            their past given to this model should not be passed as `input_ids` as they have already been computed.\n",
    "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            If `past_key_values` is used, `attention_mask` needs to contain the masking strategy that was used for\n",
    "            `past_key_values`. In other words, the `attention_mask` always has to have the length:\n",
    "            `len(past_key_values) + len(input_ids)`\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
    "            1]`:\n",
    "\n",
    "            - 0 corresponds to a *sentence A* token,\n",
    "            - 1 corresponds to a *sentence B* token.\n",
    "\n",
    "            [What are token type IDs?](../glossary#token-type-ids)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
    "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
    "            model's internal embedding lookup matrix.\n",
    "\n",
    "            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n",
    "            `past_key_values`).\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "_CHECKPOINT_FOR_DOC = \"openai-community/gpt2\"\n",
    "_CONFIG_FOR_DOC = \"GPT2Config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a277c-903a-49f7-b774-5ead54886815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTM_GPT2Model(GPT2Model):\n",
    "    \"\"\" Custom LTM GPT2 layer with memory \"\"\"\n",
    "    def __init__(self, model: GPT2Model, cnt_blocks_with_memory=2):\n",
    "        super().__init__(model.config)\n",
    "        self.base_model = model\n",
    "        \n",
    "        self.embed_dim = self.base_model.embed_dim\n",
    "\n",
    "        self.wte = self.base_model.wte\n",
    "        self.wpe = self.base_model.wpe\n",
    "\n",
    "        self.drop = self.base_model.drop\n",
    "        self.h = self.base_model.h[:-cnt_blocks_with_memory]\n",
    "        self.transformer_ltm_blocks_h = nn.ModuleList([\n",
    "            LTMGPT2Block(self.base_model.h[-cnt_blocks_with_memory+i]) for i in range(cnt_blocks_with_memory)\n",
    "        ])\n",
    "        self.ln_f = self.base_model.ln_f\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = self.base_model.model_parallel\n",
    "        self.device_map = self.base_model.device_map\n",
    "        self.gradient_checkpointing = self.base_model.gradient_checkpointing\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        # self.post_init()\n",
    "        \n",
    "    \n",
    "    \n",
    "    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.base_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.base_model.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.base_model.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.base_model.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.base_model.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "            batch_size = input_ids.shape[0]\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        if past_key_values is None:\n",
    "            past_length = 0\n",
    "            past_key_values = tuple([None] * len(self.h))\n",
    "        else:\n",
    "            past_length = past_key_values[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        # GPT2Attention mask.\n",
    "        if attention_mask is not None:\n",
    "            if batch_size <= 0:\n",
    "                raise ValueError(\"batch_size has to be defined and > 0\")\n",
    "            attention_mask = attention_mask.view(batch_size, -1)\n",
    "            # We create a 3D attention mask from a 2D tensor mask.\n",
    "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "            # this attention mask is more simple than the triangular masking of causal attention\n",
    "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "            # masked positions, this operation will create a tensor which is 0.0 for\n",
    "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "            # Since we are adding it to the raw scores before the softmax, this is\n",
    "            # effectively the same as removing these entirely.\n",
    "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.base_model.config.add_cross_attention and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask) # self.base_model.\n",
    "        else:\n",
    "            encoder_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
    "        head_mask = self.get_head_mask(head_mask, self.base_model.config.n_layer) # self.base_model.\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            token_type_embeds = self.wte(token_type_ids)\n",
    "            hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        presents = () if use_cache else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "            # Model parallel\n",
    "            if self.model_parallel:\n",
    "                torch.cuda.set_device(hidden_states.device)\n",
    "                # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
    "                if layer_past is not None:\n",
    "                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
    "                # Ensure that attention_mask is always on the same device as hidden_states\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(hidden_states.device)\n",
    "                if isinstance(head_mask, torch.Tensor):\n",
    "                    head_mask = head_mask.to(hidden_states.device)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                outputs = self._gradient_checkpointing_func(\n",
    "                    block.__call__,\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    attention_mask,\n",
    "                    head_mask[i],\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    use_cache,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                outputs = block(\n",
    "                    hidden_states,\n",
    "                    layer_past=layer_past,\n",
    "                    attention_mask=attention_mask,\n",
    "                    head_mask=head_mask[i],\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    use_cache=use_cache,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = outputs[0]\n",
    "            if use_cache is True:\n",
    "                presents = presents + (outputs[1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
    "\n",
    "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "            if self.model_parallel:\n",
    "                for k, v in self.device_map.items():\n",
    "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "        memory = hidden_states\n",
    "        \n",
    "        for i, block in enumerate(self.transformer_ltm_blocks_h): # TODO add flags like in `for` up\n",
    "            block.update_memory(memory)\n",
    "            hidden_states = block(hidden_states)\n",
    "        \n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.view(output_shape)\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6006f-8f15-4db8-9535-770fc5ffb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a8463-606e-4b0f-92a6-7eabc7dc7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90621710-05dc-4db5-a997-1420dba1458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80545898-0e60-4209-baaf-068d18c49fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer = LTM_GPT2Model(model.transformer)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7138929-7180-443f-be23-6654e3b64e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTM_GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71001b-980f-4904-b678-7db46a08d2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a986e4-cd69-4c60-a977-52d06702271a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb6ea9-3b6e-44a5-987c-3e6fd97dc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LTM_GPT(GPT2LMHeadModel):\n",
    "#     \"\"\" Custom LTM GPT2 layer with memory \"\"\"\n",
    "#     def __init__(self, model: GPT2LMHeadModel, cnt_blocks_with_memory=2):\n",
    "#         super().__init__()\n",
    "#         self.base_model = model\n",
    "#         self.transformer = self.base_model.tranformer GPT2Model(config)\n",
    "#         self.transformer.h = self.base_model.transformer.h[:-cnt_blocks_with_memory]\n",
    "        \n",
    "#         self.transformer_ltm_blocks = nn.ModuleList([\n",
    "#             LTMGPT2Block(self.base_model.transformer.h[-cnt_blocks_with_memory+i]) for i in range(cnt_blocks_with_memory)\n",
    "#         ])\n",
    "        \n",
    "#         self.lm_head = self.base_model.lm_head\n",
    "\n",
    "#         # Model parallel\n",
    "#         # self.model_parallel = False\n",
    "#         # self.device_map = None\n",
    "\n",
    "#         # Initialize weights and apply final processing\n",
    "#         # self.post_init()\n",
    "    \n",
    "#     @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
    "#     @add_code_sample_docstrings(\n",
    "#         checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "#         output_type=CausalLMOutputWithCrossAttentions,\n",
    "#         config_class=_CONFIG_FOR_DOC,\n",
    "#     )\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: Optional[torch.LongTensor] = None,\n",
    "#         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "#         attention_mask: Optional[torch.FloatTensor] = None,\n",
    "#         token_type_ids: Optional[torch.LongTensor] = None,\n",
    "#         position_ids: Optional[torch.LongTensor] = None,\n",
    "#         head_mask: Optional[torch.FloatTensor] = None,\n",
    "#         inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "#         encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "#         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "#         labels: Optional[torch.LongTensor] = None,\n",
    "#         use_cache: Optional[bool] = None,\n",
    "#         output_attentions: Optional[bool] = None,\n",
    "#         output_hidden_states: Optional[bool] = None,\n",
    "#         return_dict: Optional[bool] = None,\n",
    "#     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "#         r\"\"\"\n",
    "#         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "#             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "#             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "#             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "#         \"\"\"\n",
    "#         return_dict = return_dict if return_dict is not None else self.base_model.config.use_return_dict\n",
    "\n",
    "#         transformer_outputs = self.transformer(\n",
    "#             input_ids,\n",
    "#             past_key_values=past_key_values,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             encoder_hidden_states=encoder_hidden_states,\n",
    "#             encoder_attention_mask=encoder_attention_mask,\n",
    "#             use_cache=use_cache,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "#         hidden_states = transformer_outputs[0]\n",
    "        \n",
    "#         # Init memory as hidden_states from 37 layers\n",
    "#         hidden_states\n",
    "\n",
    "#         # Set device for model parallelism\n",
    "#         if self.base_model.model_parallel:\n",
    "#             torch.cuda.set_device(self.transformer.first_device)\n",
    "#             hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device to enable model parallelism\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             # Shift so that tokens < n predict n\n",
    "#             shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#             shift_labels = labels[..., 1:].contiguous()\n",
    "#             # Flatten the tokens\n",
    "#             loss_fct = CrossEntropyLoss()\n",
    "#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "#         if not return_dict:\n",
    "#             output = (lm_logits,) + transformer_outputs[1:]\n",
    "#             return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "#         return CausalLMOutputWithCrossAttentions(\n",
    "#             loss=loss,\n",
    "#             logits=lm_logits,\n",
    "#             past_key_values=transformer_outputs.past_key_values,\n",
    "#             hidden_states=transformer_outputs.hidden_states,\n",
    "#             attentions=transformer_outputs.attentions,\n",
    "#             cross_attentions=transformer_outputs.cross_attentions,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd2fe9-b01b-49af-a2d6-acce67398121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf155a6-1d6d-46df-b0bc-a64c1ff86c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c68a8-bb75-4577-8d61-d943fbe0add9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2c48f-01d5-426b-a83a-e79cdf01da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1ec17-979f-4e4a-859f-634d75091640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db387cf-ae03-4055-973b-751968f92ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters())\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa29fd1-b5bd-43bd-b6c1-5cb7aff8b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db82518-86b2-4da2-9897-b76d4af6d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(count_parameters(model))\n",
    "\n",
    "for param in model.transformer.transformer_ltm_blocks_h.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36544930-9703-4a1f-b37b-42eba15bb42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e8325-59cf-45be-8297-39a7a54c2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init LTM gpt blocks\n",
    "# model.transformer.h[-2] = LTMGPT2Block(model.transformer.h[-2])\n",
    "# model.transformer.h[-1] = LTMGPT2Block(model.transformer.h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e0b57-0a02-4955-98fa-9c559e8f231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upcast\n",
    "# for param in model.transformer.h[-2:].parameters():\n",
    "#     param.data = param.data.to(torch.float32)\n",
    "    \n",
    "# for param in model.transformer.ln_f.parameters():\n",
    "#     param.requires_grad=True\n",
    "#     param.data = param.data.to(torch.float32)\n",
    "\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad=True\n",
    "#     param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04654acc-abe2-477a-8099-247e407cc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899717fa-1a07-4645-9c4d-999324b7aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035fc13-efc0-40bc-97f7-8f49e62dd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruGPT-3.5-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8bf5a-7023-42cb-acca-e7d7b56ee9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90424abe-c84e-4a32-8abb-5db5bf77f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =  ['import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    print(tokenizer(prompt, return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5e785-26b8-4768-8d51-ed33d6d37695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "    print(batch)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        #print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1) #.argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        #print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        #print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "        break\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4848d2b-b134-41e2-836b-82dc45b76d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673a7a7-0663-4b02-bd4f-2fad774fbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_finetuning_samples = []\n",
    "for prompt in tqdm(prompts):\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "after_finetuning_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb45790-6e71-48cc-b76d-96b36c677ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f652b89-ed17-476c-8f7a-93215b404cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a67d7-031c-42f8-8a1d-9629e22e3bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70567fcc-d2ae-4649-a437-2d0c40bc9e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f06b5-ffb9-4f6a-89ec-5318b2a25f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1baba-f148-4eee-9b6f-50240aebba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202d09e-9dce-4f13-b04e-2cb98cda387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210ece3-cf88-4137-979f-620aee4db2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac355dc-9fa5-41a2-8a4f-b79a65feec35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce52c-3899-4372-be2b-0503ab5a20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
