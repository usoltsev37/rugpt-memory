{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d70b90-86b6-46ae-a61c-d75a6822ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita_u/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b21fd3-262f-40e8-bf08-7a1e093306e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.77s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ai-forever/ruGPT-3.5-13B\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True, \n",
    "    cache_dir=\"/home/nikita_u/study/nir/repo/rugpt-memory/checkpoints/base/huggingface/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38ca59e-0f38-4936-b167-dca71157e6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50272, 5120)\n",
       "    (wpe): Embedding(2048, 5120)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-39): 40 x GPT2Block(\n",
       "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217a923e-4686-4d6f-8eb2-63921cc283d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 5120\n",
    "x = torch.randn(batch, sentence_length, embedding_dim, requires_grad=True, dtype=torch.float16)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53d61df-40ad-43a4-8769-ab28259f4cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[39].ln_1.normalized_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1cdb8b-51b1-447e-a194-4912fdbb38fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "    (c_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "    (c_proj): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ad5cc1-c828-4352-b230-40502ff5d2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita_u/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_o \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[\u001b[38;5;241m39\u001b[39m](x)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mx_o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x_o = model.transformer.h[39](x)\n",
    "x_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2245b924-23de-41c6-b989-29f69ca92632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60c75eb-5128-4599-ad24-b9968dc042a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "  (c_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[39].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ec3d3ec-844a-46d8-bea9-11b421e3f6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=5120, out_features=5120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MultiheadAttention(embed_dim=5120, num_heads=4, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b62f3d-3665-4439-bb11-c7882a642dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=5120, out_features=5120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = nn.MultiheadAttention(embed_dim=5120, num_heads=4, dropout=0.1, dtype=torch.float16)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7589d9f-33ea-4fdd-811d-4742416e6fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c75752-a753-414d-a8cd-bc8904c112b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m o_attn \u001b[38;5;241m=\u001b[39m attn(\n\u001b[1;32m      2\u001b[0m     query\u001b[38;5;241m=\u001b[39mx, \n\u001b[1;32m      3\u001b[0m     key\u001b[38;5;241m=\u001b[39mx, \n\u001b[1;32m      4\u001b[0m     value\u001b[38;5;241m=\u001b[39mx\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mo_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "o_attn = attn(\n",
    "    query=x, \n",
    "    key=x, \n",
    "    value=x\n",
    ")\n",
    "o_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e683b4ca-ca7b-4dc4-88da-fd15b8ea3814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(o_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4524a931-8919-49f2-9217-dfad8e83f8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_attn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "521206b9-c6bf-4ffa-940b-f34b9925b31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20, 20])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_attn[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295beb6-c87f-47f9-bc89-5dfaaef6eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b9963-0d40-44ef-b153-32d11a139e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2ab27-633f-400d-8675-e8a4d4d87d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc964c2-99da-41dd-a026-14112f815c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802089c-3a3f-4718-9586-cb9452410a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aecbef-ed69-4ba9-a701-da84c973590e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378ca55-199b-457e-b9d0-1cafb3b81dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a3676e-857b-4930-822c-0fab7eb1faea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f52c0fd2-6412-4bb4-92e8-45e444e0789f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nn.Linear(5120, 5120, dtype=torch.float16)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c3500-5368-4df7-a762-16fcc928e6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea79e172-6a36-44be-97c4-df7d367a7fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5, 20480])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nn.Linear(5120, 2*5120, dtype=torch.float16)(x)\n",
    "y = nn.ReLU()(y)\n",
    "print(y.shape)\n",
    "y = nn.Linear(2*5120, 5120, dtype=torch.float16)(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a2e27-dda6-4a87-ad53-3a8f08527614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbbc30-3a0b-4462-96e1-737c7103d3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7517e60-e664-495b-8db8-d3905a0f2767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c25b93-91b7-41af-b3b7-8b47ae618ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.6125e-03,  1.3330e+00,  4.0894e-01,  ..., -1.6333e-01,\n",
       "           2.9639e-01, -3.5156e-01],\n",
       "         [ 7.3242e-02, -7.8174e-01,  6.9189e-01,  ..., -4.7168e-01,\n",
       "           6.6992e-01,  1.3662e+00],\n",
       "         [-4.0967e-01, -7.3779e-01,  4.2676e-01,  ...,  7.9590e-01,\n",
       "           4.1016e-01, -4.2432e-01],\n",
       "         [-1.1914e+00,  3.3667e-01,  2.9102e-01,  ...,  6.7871e-01,\n",
       "          -4.3359e-01,  3.1763e-01],\n",
       "         [ 6.9434e-01,  1.4526e-02, -4.5471e-02,  ..., -1.5495e-02,\n",
       "          -1.4783e-01, -1.9409e-02]],\n",
       "\n",
       "        [[ 4.6533e-01, -7.1533e-02,  5.8008e-01,  ..., -5.9473e-01,\n",
       "          -2.5977e-01,  4.3896e-01],\n",
       "         [ 5.6201e-01, -5.0439e-01, -2.9810e-01,  ...,  1.0217e-01,\n",
       "          -7.3128e-03,  9.5654e-01],\n",
       "         [ 1.2002e+00, -2.9150e-01,  6.4893e-01,  ..., -1.8787e-01,\n",
       "          -2.0312e-01, -2.8809e-01],\n",
       "         [ 1.2549e-01,  4.6313e-01,  2.3267e-01,  ...,  6.6309e-01,\n",
       "          -1.0712e-01,  6.1182e-01],\n",
       "         [-9.1699e-01,  4.3427e-02, -8.5596e-01,  ...,  9.1113e-01,\n",
       "          -9.8779e-01,  4.7095e-01]],\n",
       "\n",
       "        [[ 7.1143e-01, -5.6885e-01, -3.6816e-01,  ..., -1.3916e-01,\n",
       "           3.1689e-01, -6.8750e-01],\n",
       "         [-7.0801e-01, -6.2695e-01, -2.9980e-01,  ...,  3.2642e-01,\n",
       "           4.5898e-01, -2.0288e-01],\n",
       "         [ 6.8726e-02,  1.3342e-01, -2.7637e-01,  ..., -2.6709e-01,\n",
       "          -2.0837e-01,  3.8184e-01],\n",
       "         [ 5.0598e-02, -8.8330e-01,  2.7246e-01,  ..., -2.2449e-01,\n",
       "           4.7827e-01, -3.3911e-01],\n",
       "         [ 4.8877e-01,  3.7573e-01, -5.0342e-01,  ...,  4.8779e-01,\n",
       "           1.1416e+00,  9.1858e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.7200e-01,  4.7876e-01, -5.5273e-01,  ...,  1.1035e-01,\n",
       "          -2.8564e-01, -3.9380e-01],\n",
       "         [-7.2803e-01, -1.1176e-01,  6.7505e-02,  ..., -6.8506e-01,\n",
       "          -1.9153e-01, -7.1826e-01],\n",
       "         [-3.6060e-01,  1.5063e-01, -9.5337e-02,  ...,  1.9348e-01,\n",
       "          -5.9180e-01,  4.9805e-01],\n",
       "         [ 1.1212e-01,  5.2832e-01,  7.0215e-01,  ..., -5.7861e-01,\n",
       "          -2.9761e-01, -2.5342e-01],\n",
       "         [ 4.1943e-01,  1.2158e+00,  6.5723e-01,  ...,  5.1208e-02,\n",
       "          -5.3027e-01, -5.8643e-01]],\n",
       "\n",
       "        [[ 8.0957e-01, -1.0215e+00, -8.6572e-01,  ..., -2.6416e-01,\n",
       "          -3.2495e-01,  7.8247e-02],\n",
       "         [-7.9248e-01,  9.1260e-01, -3.0371e-01,  ...,  6.3232e-02,\n",
       "           3.5156e-01,  7.3608e-02],\n",
       "         [ 5.8984e-01, -3.9429e-01, -7.0129e-02,  ...,  8.1055e-01,\n",
       "           8.7988e-01,  3.1158e-02],\n",
       "         [-6.5625e-01,  5.2460e-02,  9.3750e-02,  ..., -1.8496e+00,\n",
       "           4.5947e-01,  9.6631e-01],\n",
       "         [-6.9389e-03,  1.4805e+00,  3.3618e-01,  ...,  3.6182e-01,\n",
       "          -2.7227e-04, -2.2424e-01]],\n",
       "\n",
       "        [[ 4.9095e-03, -7.5000e-01,  1.9336e-01,  ...,  3.7933e-02,\n",
       "          -3.7964e-01,  9.0625e-01],\n",
       "         [-2.1699e+00,  3.3350e-01,  6.2256e-01,  ..., -2.6367e-01,\n",
       "          -1.0248e-01,  3.9575e-01],\n",
       "         [ 9.3262e-02, -5.9424e-01,  1.6663e-01,  ...,  8.3594e-01,\n",
       "          -7.3535e-01,  6.4160e-01],\n",
       "         [-3.5400e-02,  1.6821e-01,  4.1840e-02,  ...,  4.3121e-02,\n",
       "          -3.2983e-01,  6.8408e-01],\n",
       "         [ 2.3364e-01,  1.1777e+00,  1.0684e+00,  ..., -2.2217e-01,\n",
       "          -8.7695e-01,  3.0664e-01]]], dtype=torch.float16,\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(5120, 5120, dtype=torch.float16)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd8afa88-6836-4db1-80ed-d1d9169dfacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92011702-15c9-4ed5-88af-7513d1891e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_level_embeddings = model.transformer.h[39].forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d8ec966-080b-4d08-b1e3-347482365925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(upper_level_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdb36f8b-4cdf-423b-83f4-29b32e6d185b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_level_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "247a844b-391e-44d2-ab0b-75e6f4f64103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a18698d-e3ca-4c14-8129-9f0832629afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 5120])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x+x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ac462e8-107d-4027-b4d0-53c604be6811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7881,  1.1523, -0.0551,  ...,  0.7070, -0.1191,  0.7812],\n",
       "         [-1.2559, -1.3057, -2.3125,  ..., -0.1284, -1.5430, -0.4556],\n",
       "         [ 0.1301,  0.6895, -0.9297,  ...,  0.1860,  1.9502, -0.7021],\n",
       "         [-0.6045,  1.0312,  0.0507,  ...,  1.4180, -0.6758, -0.0483],\n",
       "         [-1.2959, -0.1399,  0.3103,  ..., -1.4834,  0.3401, -1.2705]],\n",
       "\n",
       "        [[-2.0645, -0.9365, -1.3574,  ..., -1.3594,  0.8984,  0.3293],\n",
       "         [ 0.5850,  1.0352, -0.6978,  ..., -1.3691,  0.6421, -0.7061],\n",
       "         [ 0.9795, -1.0713, -1.1191,  ...,  1.2744,  0.3896, -1.7441],\n",
       "         [ 0.4529, -0.2864,  1.1650,  ..., -0.8154,  0.8726,  1.0547],\n",
       "         [ 0.2610,  0.3921, -0.4343,  ..., -0.5005,  0.0078,  0.5220]],\n",
       "\n",
       "        [[-0.0681,  1.4385,  0.4812,  ...,  1.2188, -1.2520, -1.0400],\n",
       "         [-0.6118,  1.1787, -0.3931,  ...,  1.1338,  0.8755,  0.1853],\n",
       "         [-1.2627,  0.6382, -0.2644,  ..., -1.0850,  0.3674, -0.6143],\n",
       "         [ 0.0480,  0.6294,  0.6533,  ...,  0.0059,  1.6172,  0.3538],\n",
       "         [ 0.1328,  0.3381, -0.0223,  ..., -0.7046,  0.5576,  0.8687]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.2031,  1.7764, -1.2061,  ..., -0.1112,  1.2920,  0.5596],\n",
       "         [-0.3281, -0.6284,  0.4993,  ..., -0.2396,  2.6738,  0.8286],\n",
       "         [ 0.0758, -0.1332,  0.2522,  ...,  0.0555,  0.6450, -1.6855],\n",
       "         [-0.0745,  1.6562, -1.4639,  ..., -0.5342,  0.4021,  1.7490],\n",
       "         [ 2.1211,  0.7119, -0.1381,  ..., -0.6343,  0.9424,  0.5840]],\n",
       "\n",
       "        [[ 0.9390, -0.3801, -0.3115,  ...,  0.1317,  0.2834, -0.3420],\n",
       "         [-0.2274,  0.6348,  0.4956,  ...,  1.5234,  1.5078, -1.0762],\n",
       "         [ 1.6836,  2.3125,  0.3508,  ...,  1.9043,  0.5454,  0.1709],\n",
       "         [ 1.7793,  0.0539,  1.5518,  ..., -0.4824, -1.9922, -0.2111],\n",
       "         [-1.3184,  0.6934,  0.2230,  ..., -1.4678, -0.2465,  1.0752]],\n",
       "\n",
       "        [[-0.0508,  0.4465, -0.1234,  ..., -0.7485,  0.7612,  1.0303],\n",
       "         [-1.4355,  0.2749, -0.9624,  ...,  0.5449,  0.2252,  0.2852],\n",
       "         [ 0.5361, -1.1338, -0.0654,  ...,  0.8994,  1.0117,  0.9463],\n",
       "         [-0.6084,  0.8242, -0.4475,  ..., -0.3928, -0.4326, -0.8701],\n",
       "         [ 0.6909, -0.2788,  0.4373,  ..., -0.4116,  0.1473, -0.6006]]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c131935-d886-493c-8ea5-3437315aaf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8096,  1.1475, -0.0701,  ...,  0.6987, -0.1348,  0.7734],\n",
       "         [-1.2646, -1.3145, -2.3164,  ..., -0.1418, -1.5508, -0.4675],\n",
       "         [ 0.1290,  0.6880, -0.9302,  ...,  0.1848,  1.9482, -0.7031],\n",
       "         [-0.6104,  1.0312,  0.0470,  ...,  1.4189, -0.6821, -0.0524],\n",
       "         [-1.2842, -0.1288,  0.3213,  ..., -1.4717,  0.3511, -1.2588]],\n",
       "\n",
       "        [[-2.0801, -0.9434, -1.3682,  ..., -1.3701,  0.9072,  0.3333],\n",
       "         [ 0.5952,  1.0459, -0.6885,  ..., -1.3604,  0.6523, -0.6968],\n",
       "         [ 0.9917, -1.0791, -1.1270,  ...,  1.2900,  0.3962, -1.7578],\n",
       "         [ 0.4624, -0.2764,  1.1738,  ..., -0.8052,  0.8818,  1.0645],\n",
       "         [ 0.2393,  0.3696, -0.4521,  ..., -0.5181, -0.0124,  0.4988]],\n",
       "\n",
       "        [[-0.0587,  1.4541,  0.4929,  ...,  1.2334, -1.2471, -1.0342],\n",
       "         [-0.5972,  1.1973, -0.3779,  ...,  1.1523,  0.8936,  0.2018],\n",
       "         [-1.2852,  0.6694, -0.2588,  ..., -1.1025,  0.3911, -0.6187],\n",
       "         [ 0.0564,  0.6387,  0.6631,  ...,  0.0141,  1.6289,  0.3628],\n",
       "         [ 0.1541,  0.3628, -0.0036,  ..., -0.6973,  0.5859,  0.9019]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.2383,  1.7666, -1.2354,  ..., -0.1337,  1.2783,  0.5415],\n",
       "         [-0.3491, -0.6528,  0.4883,  ..., -0.2595,  2.6895,  0.8218],\n",
       "         [ 0.0750, -0.1353,  0.2524,  ...,  0.0545,  0.6475, -1.6963],\n",
       "         [-0.0683,  1.6846, -1.4756,  ..., -0.5337,  0.4143,  1.7783],\n",
       "         [ 2.1465,  0.7261, -0.1305,  ..., -0.6309,  0.9585,  0.5972]],\n",
       "\n",
       "        [[ 0.9375, -0.4058, -0.3359,  ...,  0.1154,  0.2698, -0.3669],\n",
       "         [-0.2524,  0.6211,  0.4802,  ...,  1.5215,  1.5059, -1.1123],\n",
       "         [ 1.7129,  2.3418,  0.3782,  ...,  1.9336,  0.5728,  0.1981],\n",
       "         [ 1.7568,  0.0326,  1.5293,  ..., -0.5034, -2.0117, -0.2323],\n",
       "         [-1.3125,  0.6909,  0.2228,  ..., -1.4609, -0.2448,  1.0713]],\n",
       "\n",
       "        [[-0.0621,  0.4395, -0.1353,  ..., -0.7656,  0.7568,  1.0283],\n",
       "         [-1.4570,  0.2683, -0.9795,  ...,  0.5405,  0.2183,  0.2788],\n",
       "         [ 0.5386, -1.1387, -0.0656,  ...,  0.9033,  1.0156,  0.9502],\n",
       "         [-0.5815,  0.8452, -0.4214,  ..., -0.3669, -0.4067, -0.8423],\n",
       "         [ 0.7026, -0.2908,  0.4426,  ..., -0.4270,  0.1456, -0.6206]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = nn.LayerNorm(5120)\n",
    "ln(x.float()).type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a92f54d-5fba-4214-ad6b-aa101b0161f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 10])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde1bea-9238-461f-8da2-814cb1d9243a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf875b-7d0a-4d63-90a6-d55077779ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcea7bd-4386-47e5-b257-18c43e2e9971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14677c19-e33a-453a-a353-8dc36f1d0486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224ba85-33b1-4c86-af28-6db0dd7ebf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcfd939-88e9-4a5d-ae55-69adf7d730ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    \"\"\" DenseNetwork layer(FeedForward in original paper) \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim=5120,\n",
    "        hidden_size=10240, \n",
    "        dtype=torch.float16,\n",
    "        initialize_with_zeros=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.ln1 = nn.Linear(self.embed_dim, self.hidden_size, dtype=self.dtype)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(self.hidden_size, self.embed_dim, dtype=self.dtype)\n",
    "        \n",
    "        if initialize_with_zeros:\n",
    "            nn.init.zeros_(self.ln1.weight)\n",
    "            nn.init.zeros_(self.ln1.bias)\n",
    "            nn.init.zeros_(self.ln2.weight)\n",
    "            nn.init.zeros_(self.ln2.bias)\n",
    "    \n",
    "    def forward(self, x): # x: (sentence_length, batch_size, self.embed_dim)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bca5fa8c-9b7f-40d0-ab70-bb6093743885",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_full = DenseNetwork(initialize_with_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22e763a5-4be6-4c08-bd85-971e6eeb4e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_full.ln1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e242490d-3f58-47c0-a2fe-71b4b2091053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_full.ln1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69b43cde-e340-4a1d-8691-c71047fbd1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_full.ln2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e84a5a11-b77d-4c12-a8c9-53c16bf3d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_full.ln2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04cdd2-7586-4aef-b600-bb88683635cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c938080-564a-45fa-a844-ea6462076e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b85ce-b474-4a65-931e-0c8e97cd8134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b962bfb-89cd-4491-863f-cccbc9f1d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c6d19-8e43-400e-bff4-f8e7bfa4a996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c6365-934a-4e2e-843e-70659bd7d0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da9025-ebc1-4e80-9cae-888124219da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a11dc6-95c4-4d92-a3a9-453ba3d2e826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21c3b9d-d868-4da9-9576-bb0b14271371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTMGPT2Block(nn.Module):\n",
    "    \"\"\" Custom LTMGPT2Block layer with memory \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        gpt2_block,\n",
    "        num_heads=4,\n",
    "        attn_dropout=0.1,\n",
    "        dense_network_hidden_size=10240,\n",
    "        dtype=torch.float32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gpt2_block = gpt2_block\n",
    "        \n",
    "        self.embed_dim = self.gpt2_block.ln_1.normalized_shape[0]\n",
    "        self.dense_network_hidden_size = dense_network_hidden_size\n",
    "        \n",
    "        assert dtype in [torch.float16, torch.float32]\n",
    "        \n",
    "        # self.memory: ( , , ) / (target_sentence_length, batch_size, self.embed_dim) (5120) | torch.FloatTensor / nn.Embedding\n",
    "        self.memory = None\n",
    "        \n",
    "        # goal: convert memory from ( , , ) to (source_sentence_length, batch_size, self.embed_dim)\n",
    "        self.dense_network1 = DenseNetwork(\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_size=self.dense_network_hidden_size, \n",
    "            dtype=dtype,\n",
    "            initialize_with_zeros=False\n",
    "        )\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention( # TODO masked ????\n",
    "            embed_dim=self.embed_dim, \n",
    "            num_heads=num_heads, \n",
    "            dropout=attn_dropout,\n",
    "            batch_first=False,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        self.dense_network2 = DenseNetwork(\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_size=self.dense_network_hidden_size, \n",
    "            dtype=dtype,\n",
    "            initialize_with_zeros=True\n",
    "        )\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x): # x: (sentence_length, batch_size, self.embed_dim)\n",
    "        assert not self.memory\n",
    "        \n",
    "        # TransformerBlock\n",
    "        query = self.gpt2_block(x) # query: (sentence_length, batch_size, self.embed_dim)\n",
    "        residual = query\n",
    "        \n",
    "        # DenseNetowork\n",
    "        memory = self.dense_network1(self.memory)\n",
    "        \n",
    "        # MultiHead Attention\n",
    "        key, value = memory, memory\n",
    "        x, _ = self.attn(\n",
    "            query=query, \n",
    "            key=key, \n",
    "            value=value\n",
    "        )\n",
    "        \n",
    "        # Norm & Concat\n",
    "        x = x + residual\n",
    "        if self.dtype == torch.float16:\n",
    "            x = self.ln1(x.float()).type(torch.float16)\n",
    "        else:\n",
    "            x = self.ln1(x)\n",
    "        \n",
    "        # DenseNetowork initialized with zeroes\n",
    "        x = self.dense_network2(x)\n",
    "        \n",
    "        # Norm & Concat\n",
    "        x = x + residual\n",
    "        if self.dtype == torch.float16:\n",
    "            x = self.ln2(x.float()).type(torch.float16)\n",
    "        else:\n",
    "            x = self.ln2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def update_memory(new_memory):\n",
    "        self.memory = new_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a8463-606e-4b0f-92a6-7eabc7dc7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb6ea9-3b6e-44a5-987c-3e6fd97dc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTM_GPT(nn.Module):\n",
    "    \"\"\" Custom LTM GPT2 layer with memory \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model_freeze = model\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd2fe9-b01b-49af-a2d6-acce67398121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c68a8-bb75-4577-8d61-d943fbe0add9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2c48f-01d5-426b-a83a-e79cdf01da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1ec17-979f-4e4a-859f-634d75091640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db387cf-ae03-4055-973b-751968f92ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db82518-86b2-4da2-9897-b76d4af6d491",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:5\u001b[0;36m\u001b[0m\n\u001b[0;31m    for param in model.transformer.ln_f.parameters():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "for param in model.transformer.h[38:].parameters():\n",
    "        param.requires_grad=True\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "    for param in model.transformer.ln_f.parameters():\n",
    "        param.requires_grad=True\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "    for param in model.lm_head.parameters():\n",
    "        param.requires_grad=True\n",
    "        param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388875a-ddf9-43c5-9d17-0b90aa298031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478e8325-59cf-45be-8297-39a7a54c2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init LTM gpt blocks\n",
    "model.transformer.h[-2] = LTMGPT2Block(model.transformer.h[-2])\n",
    "model.transformer.h[-1] = LTMGPT2Block(model.transformer.h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf8e0b57-0a02-4955-98fa-9c559e8f231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upcast\n",
    "for param in model.transformer.h[-2:].parameters():\n",
    "    param.data = param.data.to(torch.float32)\n",
    "    \n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad=True\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad=True\n",
    "    param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04654acc-abe2-477a-8099-247e407cc6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3035fc13-efc0-40bc-97f7-8f49e62dd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruGPT-3.5-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e8bf5a-7023-42cb-acca-e7d7b56ee9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita_u/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "code_dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90424abe-c84e-4a32-8abb-5db5bf77f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 378.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[33076]]), 'attention_mask': tensor([[1]])}\n",
      "{'input_ids': tensor([[34958]]), 'attention_mask': tensor([[1]])}\n",
      "{'input_ids': tensor([[29631]]), 'attention_mask': tensor([[1]])}\n",
      "{'input_ids': tensor([[  89, 2286]]), 'attention_mask': tensor([[1, 1]])}\n",
      "{'input_ids': tensor([[1271]]), 'attention_mask': tensor([[1]])}\n",
      "{'input_ids': tensor([[9949]]), 'attention_mask': tensor([[1]])}\n",
      "{'input_ids': tensor([[23652,  1028]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts =  ['import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    print(tokenizer(prompt, return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a5e785-26b8-4768-8d51-ed33d6d37695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        #print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1) #.argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        #print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        #print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0673a7a7-0663-4b02-bd4f-2fad774fbfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m after_finetuning_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(prompts):\n\u001b[0;32m----> 3\u001b[0m     after_finetuning_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcustom_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m after_finetuning_samples\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mcustom_generate\u001b[0;34m(prompt, model, device, max_steps)\u001b[0m\n\u001b[1;32m      2\u001b[0m batch \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     probs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnan_to_num(nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#.argmax(-1).reshape(1, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[1;32m    388\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 389\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    391\u001b[0m         hidden_states,\n\u001b[1;32m    392\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rugpt_dev/lib/python3.9/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Half"
     ]
    }
   ],
   "source": [
    "after_finetuning_samples = []\n",
    "for prompt in tqdm(prompts):\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "after_finetuning_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb45790-6e71-48cc-b76d-96b36c677ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f652b89-ed17-476c-8f7a-93215b404cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a67d7-031c-42f8-8a1d-9629e22e3bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70567fcc-d2ae-4649-a437-2d0c40bc9e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f06b5-ffb9-4f6a-89ec-5318b2a25f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1baba-f148-4eee-9b6f-50240aebba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202d09e-9dce-4f13-b04e-2cb98cda387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210ece3-cf88-4137-979f-620aee4db2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac355dc-9fa5-41a2-8a4f-b79a65feec35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce52c-3899-4372-be2b-0503ab5a20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
