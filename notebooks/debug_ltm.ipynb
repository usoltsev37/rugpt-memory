{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d70b90-86b6-46ae-a61c-d75a6822ff88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T11:27:21.982414Z",
     "start_time": "2024-03-21T11:27:19.411012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 10:28:13 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:19:00.0 Off |                  N/A |\n",
      "| 30%   36C    P5              96W / 350W |     10MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   29C    P5              49W / 350W |     10MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      1597      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc2a266-40ce-4575-b93c-e16ced929bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import notebook_launcher\n",
    "# from accelerate imporself.model_.t Accelerator\n",
    "\n",
    "# def test_loop():\n",
    "#     accelerator = Accelerator()\n",
    "#     print(accelerator.state)\n",
    "# notebook_launcher(test_loop, [], num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807b865f-ad98-4067-a15d-894d482bc9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/usoltsev/study/repositories/rugpt-memory/')\n",
    "\n",
    "# assert torch.cuda.is_available()\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b28f573-e460-489b-817c-82f2700caf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ltm_gpt.ltm_gpt import LTM_GPT\n",
    "from src.utils.train_config import load_config\n",
    "from src.models.load_base_model import load_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4d12bf-62e4-466f-93f8-8b6fe7db15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3a57ad-b141-40e8-b1d5-2d5734b2105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-04-01 10:28:16,032: INFO] Loading base model (load_base_model.py:9)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:08<00:00,  1.37s/it]\n",
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/usoltsev/miniforge3/envs/rugpt_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "main_config = load_config('/home/usoltsev/study/repositories/rugpt-memory/configs/finetuning_codeparrot.yml')\n",
    "\n",
    "model, tokenizer = load_base_model(main_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0525392-8a6a-49b1-839a-ffab62f445b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50272, 5120)\n",
       "    (wpe): Embedding(2048, 5120)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-39): 40 x GPT2Block(\n",
       "        (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): CastOutputToFloat(\n",
       "    (0): Linear(in_features=5120, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0882ac-6150-4b39-86d0-a3037b03296c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c04d9d3b-231e-4b66-87a9-a539a25c503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ea318f8-8835-463d-96b8-62935a0a693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_gpt2_block = LTMGPT2Block(model.transformer.h[0])\n",
    "ltm_gpt2_block = ltm_gpt2_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcdab3b9-8c64-4516-949a-af60e69cbad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm_gpt2_block.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26272209-633e-4b47-a4f3-fb07043c751f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 5120])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((10, 4, ltm_gpt2_block.embed_dim), dtype=torch.float16) # x: (sentence_length, batch_size, self.embed_dim)\n",
    "x = x.to(device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "874c7d85-67f0-441c-ac64-7b25924d54a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 5120])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = torch.rand((10, 4, ltm_gpt2_block.embed_dim), dtype=torch.float16) # x: (target_sentence_length, batch_size, self.embed_dim)\n",
    "memory = memory.to(device)\n",
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "63ab4732-ba3f-40bf-9895-2c6665be95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1e9f53d-fb27-4c2d-aa79-819be1cfba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_gpt2_block.update_memory(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e33794ef-1a5f-4ac8-9bdf-9104dc990413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(query): <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "y = ltm_gpt2_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "caf2756b-67a3-407d-885c-d61040c5c114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318dd44-c3ca-4618-896b-1e3b7ae77c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18407a95-05d6-44f6-963f-136155bef480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90adb877-3940-40a5-a5cc-c9ed65a61235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ca4992e-caa7-4d51-a9f0-348b3bb78c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe016eb5-ebb7-45b3-a067-076a8ebda6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "924e24cd-18cd-449a-9120-ae21e3f8aa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2bf4176d-843a-4cd6-adca-b2c930747ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.float().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb76751-09e8-48cd-b83b-699886da93a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ceaafd-b869-4396-b8a3-6cb8f3820f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (torch.Tensor(1), torch.Tensor(2), torch.Tensor(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aceba0c9-ff83-45b9-8537-1cf524c250eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.Tensor(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb77c3f-e1a4-4d3a-b2f2-601cdcda7d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1] at entry 0 and [2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1] at entry 0 and [2] at entry 1"
     ]
    }
   ],
   "source": [
    "torch.stack(list(q), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23766a7-980c-4442-9ee2-4acda784fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1f0a6-4596-41bc-83c2-b17fb5bbda04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dc91e-adb9-4736-9ee4-bbfbaf55d9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c8baa-2179-45db-9f00-5792d79e734e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96912b06-4e7a-4453-8769-9a0d8601fdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764d9dc-74e4-4595-b6ff-68ebb161d633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2531ea-f34b-4039-86eb-d65b1064db85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055e909-07c8-491c-a6b8-e8d470666f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838565d-6d81-438f-a0e6-3bdb1e3df19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35a979-bb5b-449d-9630-25c4d688289c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd583465-0d85-4ccb-8d3a-8af7187ed4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653cf5f-9dda-4cc8-aa0a-6716ed36bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65df8fa-112e-4ce7-8fc3-902d4ffae236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from typing import Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca888cf-b28a-42ec-a34b-17973ed45244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b11797-eb55-40d9-ab70-8cf0fac86caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f4cc8-11dd-4f02-abfe-f1f4850a512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3530d76-79da-48f2-b3e0-979d0dc28688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LTM_GPT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e18dd6-6c01-45b3-bcfd-6ebc8dfdcf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac44d17-c996-4a8b-99ef-f74ce23451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruGPT-3.5-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e589cab-0e0d-49cc-94e1-150bd924db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611d769-a31d-46bf-a574-cd6711eac9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =  ['import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    print(tokenizer(prompt, return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1e103-a121-44e0-ac73-54ae58d2734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "    print(batch)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        #print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1) #.argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        #print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        #print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "        break\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b26bc3-3763-4e2e-ab99-253ab90690e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_finetuning_samples = []\n",
    "for prompt in tqdm(prompts):\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "after_finetuning_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cb81d-971c-4240-8aa6-1ce689915863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427b55b-1040-4532-b0b3-8e12810c21a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be29ade-27b2-485a-9b05-60f4c4c5b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330d6bb-50f3-4a8f-85ff-d141c88ba309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5bf2d-e0b7-461e-9773-624b5f343856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e97d3f-f8eb-4eb4-8346-1ebf58555c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0f4b6-f149-4ff0-90d1-d40e514278af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03486b6-ae21-4696-9656-019798ab9c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ec581-c299-4f51-8468-f84c377e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f7938-6460-4055-820f-80905cc302f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebee22e-8aaf-48c9-b8b2-34dda72ffa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec01194-2d55-41e2-97f4-1a7730ffb255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289ae8-1011-4422-b1e6-d5c69eef5642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95210ae0-4f18-4e33-abe5-23016e7c0be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046e5a0-4dba-450d-adc8-b7158d297805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa818dbd-d310-4aca-b57b-c1c4e265b911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b669a-7646-4d8a-8fa8-c0b6d429ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bdad0-d8c3-47c9-8665-5a1a83a02783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036148b-45f2-49fc-9df4-81059f54f54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136f161-f03d-43c4-8227-0ae0a64ea049",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-3 + i for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c220098-1b3c-4ca7-93a5-c5e650817e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from typing import Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31024f97-4192-4b1b-958b-111b6112b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n",
    "            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n",
    "            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n",
    "            sequence tokens in the vocabulary.\n",
    "\n",
    "            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n",
    "            `input_ids`.\n",
    "\n",
    "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        past_key_values (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`):\n",
    "            Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n",
    "            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n",
    "            their past given to this model should not be passed as `input_ids` as they have already been computed.\n",
    "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            If `past_key_values` is used, `attention_mask` needs to contain the masking strategy that was used for\n",
    "            `past_key_values`. In other words, the `attention_mask` always has to have the length:\n",
    "            `len(past_key_values) + len(input_ids)`\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
    "            1]`:\n",
    "\n",
    "            - 0 corresponds to a *sentence A* token,\n",
    "            - 1 corresponds to a *sentence B* token.\n",
    "\n",
    "            [What are token type IDs?](../glossary#token-type-ids)\n",
    "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
    "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
    "            model's internal embedding lookup matrix.\n",
    "\n",
    "            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n",
    "            `past_key_values`).\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "_CHECKPOINT_FOR_DOC = \"openai-community/gpt2\"\n",
    "_CONFIG_FOR_DOC = \"GPT2Config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a277c-903a-49f7-b774-5ead54886815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTM_GPT2Model(GPT2Model):\n",
    "    \"\"\" Custom LTM GPT2 layer with memory \"\"\"\n",
    "    def __init__(self, model: GPT2Model, cnt_blocks_with_memory=2):\n",
    "        super().__init__(model.config)\n",
    "        self.base_model = model\n",
    "        \n",
    "        self.embed_dim = self.base_model.embed_dim\n",
    "\n",
    "        self.wte = self.base_model.wte\n",
    "        self.wpe = self.base_model.wpe\n",
    "\n",
    "        self.drop = self.base_model.drop\n",
    "        self.h = self.base_model.h[:-cnt_blocks_with_memory]\n",
    "        self.transformer_ltm_blocks_h = nn.ModuleList([\n",
    "            LTMGPT2Block(self.base_model.h[-cnt_blocks_with_memory+i]) for i in range(cnt_blocks_with_memory)\n",
    "        ])\n",
    "        self.ln_f = self.base_model.ln_f\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = self.base_model.model_parallel\n",
    "        self.device_map = self.base_model.device_map\n",
    "        self.gradient_checkpointing = self.base_model.gradient_checkpointing\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        # self.post_init()\n",
    "        \n",
    "    \n",
    "    \n",
    "    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.base_model.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.base_model.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.base_model.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.base_model.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.base_model.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "            batch_size = input_ids.shape[0]\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        if past_key_values is None:\n",
    "            past_length = 0\n",
    "            past_key_values = tuple([None] * len(self.h))\n",
    "        else:\n",
    "            past_length = past_key_values[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        # GPT2Attention mask.\n",
    "        if attention_mask is not None:\n",
    "            if batch_size <= 0:\n",
    "                raise ValueError(\"batch_size has to be defined and > 0\")\n",
    "            attention_mask = attention_mask.view(batch_size, -1)\n",
    "            # We create a 3D attention mask from a 2D tensor mask.\n",
    "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "            # this attention mask is more simple than the triangular masking of causal attention\n",
    "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "            # masked positions, this operation will create a tensor which is 0.0 for\n",
    "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "            # Since we are adding it to the raw scores before the softmax, this is\n",
    "            # effectively the same as removing these entirely.\n",
    "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.base_model.config.add_cross_attention and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask) # self.base_model.\n",
    "        else:\n",
    "            encoder_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
    "        head_mask = self.get_head_mask(head_mask, self.base_model.config.n_layer) # self.base_model.\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            token_type_embeds = self.wte(token_type_ids)\n",
    "            hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        presents = () if use_cache else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "            # Model parallel\n",
    "            if self.model_parallel:\n",
    "                torch.cuda.set_device(hidden_states.device)\n",
    "                # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
    "                if layer_past is not None:\n",
    "                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
    "                # Ensure that attention_mask is always on the same device as hidden_states\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(hidden_states.device)\n",
    "                if isinstance(head_mask, torch.Tensor):\n",
    "                    head_mask = head_mask.to(hidden_states.device)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                outputs = self._gradient_checkpointing_func(\n",
    "                    block.__call__,\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    attention_mask,\n",
    "                    head_mask[i],\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    use_cache,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                outputs = block(\n",
    "                    hidden_states,\n",
    "                    layer_past=layer_past,\n",
    "                    attention_mask=attention_mask,\n",
    "                    head_mask=head_mask[i],\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    use_cache=use_cache,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = outputs[0]\n",
    "            if use_cache is True:\n",
    "                presents = presents + (outputs[1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
    "\n",
    "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "            if self.model_parallel:\n",
    "                for k, v in self.device_map.items():\n",
    "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "        memory = hidden_states\n",
    "        \n",
    "        for i, block in enumerate(self.transformer_ltm_blocks_h): # TODO add flags like in `for` up\n",
    "            block.update_memory(memory)\n",
    "            hidden_states = block(hidden_states)\n",
    "        \n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.view(output_shape)\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6006f-8f15-4db8-9535-770fc5ffb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a8463-606e-4b0f-92a6-7eabc7dc7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90621710-05dc-4db5-a997-1420dba1458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80545898-0e60-4209-baaf-068d18c49fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer = LTM_GPT2Model(model.transformer)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7138929-7180-443f-be23-6654e3b64e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTM_GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71001b-980f-4904-b678-7db46a08d2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a986e4-cd69-4c60-a977-52d06702271a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb6ea9-3b6e-44a5-987c-3e6fd97dc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LTM_GPT(GPT2LMHeadModel):\n",
    "#     \"\"\" Custom LTM GPT2 layer with memory \"\"\"\n",
    "#     def __init__(self, model: GPT2LMHeadModel, cnt_blocks_with_memory=2):\n",
    "#         super().__init__()\n",
    "#         self.base_model = model\n",
    "#         self.transformer = self.base_model.tranformer GPT2Model(config)\n",
    "#         self.transformer.h = self.base_model.transformer.h[:-cnt_blocks_with_memory]\n",
    "        \n",
    "#         self.transformer_ltm_blocks = nn.ModuleList([\n",
    "#             LTMGPT2Block(self.base_model.transformer.h[-cnt_blocks_with_memory+i]) for i in range(cnt_blocks_with_memory)\n",
    "#         ])\n",
    "        \n",
    "#         self.lm_head = self.base_model.lm_head\n",
    "\n",
    "#         # Model parallel\n",
    "#         # self.model_parallel = False\n",
    "#         # self.device_map = None\n",
    "\n",
    "#         # Initialize weights and apply final processing\n",
    "#         # self.post_init()\n",
    "    \n",
    "#     @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
    "#     @add_code_sample_docstrings(\n",
    "#         checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "#         output_type=CausalLMOutputWithCrossAttentions,\n",
    "#         config_class=_CONFIG_FOR_DOC,\n",
    "#     )\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: Optional[torch.LongTensor] = None,\n",
    "#         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "#         attention_mask: Optional[torch.FloatTensor] = None,\n",
    "#         token_type_ids: Optional[torch.LongTensor] = None,\n",
    "#         position_ids: Optional[torch.LongTensor] = None,\n",
    "#         head_mask: Optional[torch.FloatTensor] = None,\n",
    "#         inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "#         encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "#         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "#         labels: Optional[torch.LongTensor] = None,\n",
    "#         use_cache: Optional[bool] = None,\n",
    "#         output_attentions: Optional[bool] = None,\n",
    "#         output_hidden_states: Optional[bool] = None,\n",
    "#         return_dict: Optional[bool] = None,\n",
    "#     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "#         r\"\"\"\n",
    "#         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "#             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "#             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "#             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "#         \"\"\"\n",
    "#         return_dict = return_dict if return_dict is not None else self.base_model.config.use_return_dict\n",
    "\n",
    "#         transformer_outputs = self.transformer(\n",
    "#             input_ids,\n",
    "#             past_key_values=past_key_values,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             encoder_hidden_states=encoder_hidden_states,\n",
    "#             encoder_attention_mask=encoder_attention_mask,\n",
    "#             use_cache=use_cache,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "#         hidden_states = transformer_outputs[0]\n",
    "        \n",
    "#         # Init memory as hidden_states from 37 layers\n",
    "#         hidden_states\n",
    "\n",
    "#         # Set device for model parallelism\n",
    "#         if self.base_model.model_parallel:\n",
    "#             torch.cuda.set_device(self.transformer.first_device)\n",
    "#             hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device to enable model parallelism\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             # Shift so that tokens < n predict n\n",
    "#             shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#             shift_labels = labels[..., 1:].contiguous()\n",
    "#             # Flatten the tokens\n",
    "#             loss_fct = CrossEntropyLoss()\n",
    "#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "#         if not return_dict:\n",
    "#             output = (lm_logits,) + transformer_outputs[1:]\n",
    "#             return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "#         return CausalLMOutputWithCrossAttentions(\n",
    "#             loss=loss,\n",
    "#             logits=lm_logits,\n",
    "#             past_key_values=transformer_outputs.past_key_values,\n",
    "#             hidden_states=transformer_outputs.hidden_states,\n",
    "#             attentions=transformer_outputs.attentions,\n",
    "#             cross_attentions=transformer_outputs.cross_attentions,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd2fe9-b01b-49af-a2d6-acce67398121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf155a6-1d6d-46df-b0bc-a64c1ff86c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c68a8-bb75-4577-8d61-d943fbe0add9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2c48f-01d5-426b-a83a-e79cdf01da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1ec17-979f-4e4a-859f-634d75091640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db387cf-ae03-4055-973b-751968f92ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters())\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa29fd1-b5bd-43bd-b6c1-5cb7aff8b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db82518-86b2-4da2-9897-b76d4af6d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(count_parameters(model))\n",
    "\n",
    "for param in model.transformer.transformer_ltm_blocks_h.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad=True\n",
    "    # param.data = param.data.to(torch.float32)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36544930-9703-4a1f-b37b-42eba15bb42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e8325-59cf-45be-8297-39a7a54c2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init LTM gpt blocks\n",
    "# model.transformer.h[-2] = LTMGPT2Block(model.transformer.h[-2])\n",
    "# model.transformer.h[-1] = LTMGPT2Block(model.transformer.h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e0b57-0a02-4955-98fa-9c559e8f231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upcast\n",
    "# for param in model.transformer.h[-2:].parameters():\n",
    "#     param.data = param.data.to(torch.float32)\n",
    "    \n",
    "# for param in model.transformer.ln_f.parameters():\n",
    "#     param.requires_grad=True\n",
    "#     param.data = param.data.to(torch.float32)\n",
    "\n",
    "# for param in model.lm_head.parameters():\n",
    "#     param.requires_grad=True\n",
    "#     param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04654acc-abe2-477a-8099-247e407cc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899717fa-1a07-4645-9c4d-999324b7aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035fc13-efc0-40bc-97f7-8f49e62dd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruGPT-3.5-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8bf5a-7023-42cb-acca-e7d7b56ee9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90424abe-c84e-4a32-8abb-5db5bf77f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts =  ['import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
    "\n",
    "MAX_STEPS = 100\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    print(tokenizer(prompt, return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5e785-26b8-4768-8d51-ed33d6d37695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generate(prompt, model, device, max_steps):\n",
    "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "    print(batch)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        outputs = model(**batch)\n",
    "        #print(outputs)\n",
    "        probs = outputs.logits[0, -1].nan_to_num(nan=0.0).div(0.8).softmax(-1) #.argmax(-1).reshape(1, 1)\n",
    "        old_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        #print(old_token)\n",
    "        next_token = torch.multinomial(probs, 1).reshape(1, 1)\n",
    "        #print(next_token)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "        break\n",
    "\n",
    "    return tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4848d2b-b134-41e2-836b-82dc45b76d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673a7a7-0663-4b02-bd4f-2fad774fbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_finetuning_samples = []\n",
    "for prompt in tqdm(prompts):\n",
    "    after_finetuning_samples.append(custom_generate(prompt, model, device, MAX_STEPS))\n",
    "after_finetuning_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb45790-6e71-48cc-b76d-96b36c677ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f652b89-ed17-476c-8f7a-93215b404cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a67d7-031c-42f8-8a1d-9629e22e3bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70567fcc-d2ae-4649-a437-2d0c40bc9e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f06b5-ffb9-4f6a-89ec-5318b2a25f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1baba-f148-4eee-9b6f-50240aebba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202d09e-9dce-4f13-b04e-2cb98cda387c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210ece3-cf88-4137-979f-620aee4db2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac355dc-9fa5-41a2-8a4f-b79a65feec35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce52c-3899-4372-be2b-0503ab5a20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
